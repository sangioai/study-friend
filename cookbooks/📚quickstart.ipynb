{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 📚StudyFriend QuickStart Guide\n",
        "In this guide will see how to setup and use 📚StudyFriend on Colab."
      ],
      "metadata": {
        "id": "S9B85SZ_gA19"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clone 📚StudyFriend\n",
        "Let's clone the repository."
      ],
      "metadata": {
        "id": "1YVm-VVJgIUK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/sangioai/study-friend"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8syjuAOF-HxA",
        "outputId": "85aa5b9e-f9f1-4973-8225-097c4be0fd29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'study-friend'...\n",
            "remote: Enumerating objects: 116, done.\u001b[K\n",
            "remote: Counting objects: 100% (116/116), done.\u001b[K\n",
            "remote: Compressing objects: 100% (99/99), done.\u001b[K\n",
            "remote: Total 116 (delta 49), reused 64 (delta 16), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (116/116), 2.37 MiB | 5.03 MiB/s, done.\n",
            "Resolving deltas: 100% (49/49), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install 📚StudyFriend dependecies\n",
        "Let's add poppler libraries to the systema path."
      ],
      "metadata": {
        "id": "VUFqMSxAgK2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install poppler-utils"
      ],
      "metadata": {
        "id": "l6J5gTZjTKyY",
        "outputId": "dc18db40-2f5f-4df3-ea43-96466a274af5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 22 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 696 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.6 [186 kB]\n",
            "Fetched 186 kB in 1s (183 kB/s)\n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 124935 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.6_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.6) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.6) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install 📚StudyFriend"
      ],
      "metadata": {
        "id": "HbFoe9DJhPgp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ./study-friend"
      ],
      "metadata": {
        "id": "hZp7rmN5SCMs",
        "outputId": "5825a820-d4ed-41be-b045-892cac86cba3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing ./study-friend\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.67.1 in /usr/local/lib/python3.11/dist-packages (from study_friend==1.0.0) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from study_friend==1.0.0) (1.26.4)\n",
            "Collecting pdf2image>=1.17.0 (from study_friend==1.0.0)\n",
            "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: natsort>=8.4.0 in /usr/local/lib/python3.11/dist-packages (from study_friend==1.0.0) (8.4.0)\n",
            "Requirement already satisfied: regex>=2024.4.16 in /usr/local/lib/python3.11/dist-packages (from study_friend==1.0.0) (2024.11.6)\n",
            "Collecting transformers>=4.49.0 (from study_friend==1.0.0)\n",
            "  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bitsandbytes>=0.45.2 (from study_friend==1.0.0)\n",
            "  Downloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes>=0.45.2->study_friend==1.0.0) (2.5.1+cu124)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from pdf2image>=1.17.0->study_friend==1.0.0) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers>=4.49.0->study_friend==1.0.0) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.49.0->study_friend==1.0.0) (0.28.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.49.0->study_friend==1.0.0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.49.0->study_friend==1.0.0) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=4.49.0->study_friend==1.0.0) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.49.0->study_friend==1.0.0) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.49.0->study_friend==1.0.0) (0.5.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers>=4.49.0->study_friend==1.0.0) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers>=4.49.0->study_friend==1.0.0) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes>=0.45.2->study_friend==1.0.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes>=0.45.2->study_friend==1.0.0) (3.1.5)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes>=0.45.2->study_friend==1.0.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes>=0.45.2->study_friend==1.0.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes>=0.45.2->study_friend==1.0.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=2.0->bitsandbytes>=0.45.2->study_friend==1.0.0)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=2.0->bitsandbytes>=0.45.2->study_friend==1.0.0)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=2.0->bitsandbytes>=0.45.2->study_friend==1.0.0)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=2.0->bitsandbytes>=0.45.2->study_friend==1.0.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=2.0->bitsandbytes>=0.45.2->study_friend==1.0.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=2.0->bitsandbytes>=0.45.2->study_friend==1.0.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes>=0.45.2->study_friend==1.0.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes>=0.45.2->study_friend==1.0.0) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes>=0.45.2->study_friend==1.0.0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes>=0.45.2->study_friend==1.0.0) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes>=0.45.2->study_friend==1.0.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes>=0.45.2->study_friend==1.0.0) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.49.0->study_friend==1.0.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.49.0->study_friend==1.0.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.49.0->study_friend==1.0.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.49.0->study_friend==1.0.0) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes>=0.45.2->study_friend==1.0.0) (3.0.2)\n",
            "Downloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
            "Downloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m124.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m110.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m839.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m91.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: study_friend\n",
            "  Building wheel for study_friend (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for study_friend: filename=study_friend-1.0.0-py3-none-any.whl size=18282 sha256=13a0becd557073ecadd842f7b6bfe283c8d2cb027fcd88777442aee8d627bd9a\n",
            "  Stored in directory: /root/.cache/pip/wheels/e4/8b/7d/82836c1b7656f27dbe0dc9b39a5f3a09ae64146b6b5199ea0c\n",
            "Successfully built study_friend\n",
            "Installing collected packages: pdf2image, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, transformers, bitsandbytes, study_friend\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.48.3\n",
            "    Uninstalling transformers-4.48.3:\n",
            "      Successfully uninstalled transformers-4.48.3\n",
            "Successfully installed bitsandbytes-0.45.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pdf2image-1.17.0 study_friend-1.0.0 transformers-4.49.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Q&A with 📚StudyFriend\n",
        "\n",
        "You'll see different steps:\n",
        "- pdf conversion\n",
        "- model downloading (if not previously cached)\n",
        "- model inference for Q&A generation, saving on temporary file\n",
        "- final output generation via markdown beautifier"
      ],
      "metadata": {
        "id": "DAMl5247LTb-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m study_friend.query -d ./study-friend/samples --verbose -o outputQA.md"
      ],
      "metadata": {
        "id": "eY0KURnISay1",
        "outputId": "4267805c-a9b3-460d-b140-bf85a8e9557b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-02-23 10:18:50.608531: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1740305930.628265    2978 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1740305930.634368    2978 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-02-23 10:18:50.654430: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Options:\n",
            "dir : ./\n",
            "image_size : 500\n",
            "output_file : output.md\n",
            "model : unsloth/Qwen2.5-VL-7B-Instruct-unsloth-bnb-4bit\n",
            "engine : transformers\n",
            "title_prompt : \n",
            "Use the Format: Title - Subtitle\n",
            "Example: Impressionism - Paintings\n",
            "\n",
            "What is the title and subtitle (if not present leave it blank) of the slide? Use the format in the example above.\n",
            "question_prompt : \n",
            "Example:\n",
            "### Slide <Slide's number>: <Slide's title>\n",
            "\n",
            "\n",
            "1. <question 1>?\n",
            "2. <question 2>?\n",
            "\n",
            "What is the subject of the slides? Generate me 2 different questions for each slide about the charts and concepts, don't provide any answer. Use the template of the example above for each slide.\n",
            "answer_prompt : \n",
            "Look at the images and briefly answer the following question:\n",
            "{question}\n",
            "group_size : 3\n",
            "temperature : 0.1\n",
            "max_tokens : 999\n",
            "singular_injectors : ['1', '']\n",
            "plural_injectors : [\"<Slide's number>\", 'for each slide']\n",
            "verbose : True\n",
            "Pdf2images: 100% 4/4 [00:07<00:00,  1.85s/it]\n",
            "Loading unsloth/Qwen2.5-VL-7B-Instruct-unsloth-bnb-4bit using transformers engine\n",
            "Parent model path of unsloth/Qwen2.5-VL-7B-Instruct-unsloth-bnb-4bit is: unsloth/Qwen2.5-VL-7B-Instruct\n",
            "model.safetensors: 100% 6.21G/6.21G [02:27<00:00, 42.1MB/s]\n",
            "generation_config.json: 100% 272/272 [00:00<00:00, 2.24MB/s]\n",
            "preprocessor_config.json: 100% 575/575 [00:00<00:00, 4.53MB/s]\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "tokenizer_config.json: 100% 7.33k/7.33k [00:00<00:00, 48.2MB/s]\n",
            "vocab.json: 100% 2.78M/2.78M [00:00<00:00, 9.19MB/s]\n",
            "merges.txt: 100% 1.67M/1.67M [00:00<00:00, 13.0MB/s]\n",
            "tokenizer.json: 100% 11.4M/11.4M [00:00<00:00, 145MB/s]\n",
            "added_tokens.json: 100% 605/605 [00:00<00:00, 6.44MB/s]\n",
            "special_tokens_map.json: 100% 614/614 [00:00<00:00, 6.45MB/s]\n",
            "chat_template.json: 100% 1.05k/1.05k [00:00<00:00, 10.3MB/s]\n",
            "generation_config.json: 100% 272/272 [00:00<00:00, 2.55MB/s]\n",
            "Documents:   0% 0/1 [00:00<?, ?it/s]Grouping images in ./presentation:\n",
            "Text prompt : <|im_start|>system\n",
            "You are a helpful assistant.<|im_end|>\n",
            "<|im_start|>user\n",
            "<|vision_start|><|image_pad|><|vision_end|>\n",
            "Use the Format: Title - Subtitle\n",
            "Example: Impressionism - Paintings\n",
            "\n",
            "What is the title and subtitle (if not present leave it blank) of the slide? Use the format in the example above.<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "Model title response: CUDAification - Optimization of PwPA kernels\n",
            "\n",
            "Questions:   0% 0/8 [00:00<?, ?it/s]\u001b[AQuerying model on: ['./presentation/0.jpeg', './presentation/1.jpeg', './presentation/2.jpeg']\n",
            "\n",
            "\n",
            "Answers  :   0% 0/14 [00:00<?, ?it/s]\u001b[A\u001b[AText prompt : <|im_start|>system\n",
            "You are a helpful assistant.<|im_end|>\n",
            "<|im_start|>user\n",
            "<|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|>\n",
            "Look at the images and briefly answer the following question:\n",
            "1. What does the term \"CUDAification\" refer to in the context of optimizing PwPA kernels?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "Model question: 1. What does the term \"CUDAification\" refer to in the context of optimizing PwPA kernels?\n",
            "Model answer: The term \"CUDAification\" refers to the process of adapting or converting existing code, specifically the PwPA kernels, for execution on NVIDIA GPUs using CUDA technology. This involves rewriting the code to take advantage of GPU-specific features like parallel processing, shared memory, and thread synchronization, which can significantly improve performance compared to running on CPUs.\n",
            "\n",
            "\n",
            "Answers  :  21% 3/14 [00:09<00:36,  3.32s/it]\u001b[A\u001b[AText prompt : <|im_start|>system\n",
            "You are a helpful assistant.<|im_end|>\n",
            "<|im_start|>user\n",
            "<|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|>\n",
            "Look at the images and briefly answer the following question:\n",
            "2. How does the optimization of PwPA kernels contribute to the performance of CUDA applications?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "Model question: 2. How does the optimization of PwPA kernels contribute to the performance of CUDA applications?\n",
            "Model answer: The optimization of PwPA (Parallel Work Partitioning Algorithm) kernels contributes to the performance of CUDA applications by improving the efficiency of data processing on GPU hardware. By optimizing these kernels, the application can better utilize the computational resources available on the GPU, leading to faster execution times and improved overall performance. This is particularly important for applications that involve large amounts of data or require high computational power, as it allows them to take full advantage of the parallel processing capabilities of GPUs.\n",
            "\n",
            "\n",
            "Answers  :  29% 4/14 [00:22<01:02,  6.25s/it]\u001b[A\u001b[AText prompt : <|im_start|>system\n",
            "You are a helpful assistant.<|im_end|>\n",
            "<|im_start|>user\n",
            "<|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|>\n",
            "Look at the images and briefly answer the following question:\n",
            "1. What key differences between CPU and GPU hardware are highlighted in this slide?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "Model question: 1. What key differences between CPU and GPU hardware are highlighted in this slide?\n",
            "Model answer: The slide highlights several key differences between CPU and GPU hardware:\n",
            "\n",
            "1. **Number of Cores**: GPUs have significantly more cores than CPUs, which allows them to perform many calculations simultaneously.\n",
            "2. **Cache Size**: GPUs typically have smaller caches compared to CPUs, but they often have multiple levels of cache (L1, L2, and sometimes L3) that can be shared among multiple cores.\n",
            "3. **Memory**: GPUs have dedicated memory (DRAM) for storing data, which is separate from the CPU's memory. This allows for faster access to data during computations.\n",
            "\n",
            "These differences make GPUs well-suited for parallel processing tasks, while CPUs excel at sequential processing.\n",
            "\n",
            "\n",
            "Answers  :  57% 8/14 [00:37<00:28,  4.73s/it]\u001b[A\u001b[AText prompt : <|im_start|>system\n",
            "You are a helpful assistant.<|im_end|>\n",
            "<|im_start|>user\n",
            "<|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|>\n",
            "Look at the images and briefly answer the following question:\n",
            "2. How do the L1, L2, and L3 caches differ in their roles within a CPU versus a GPU architecture?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "Model question: 2. How do the L1, L2, and L3 caches differ in their roles within a CPU versus a GPU architecture?\n",
            "Model answer: In a CPU architecture, the L1 cache is the smallest and fastest cache, located closest to the CPU core, used for storing frequently accessed data. The L2 cache is larger than the L1 cache but still relatively small compared to the main memory (DRAM). It serves as a buffer between the CPU core and DRAM. The L3 cache is the largest cache in a CPU architecture, located between the CPU cores and the DRAM, providing a faster access point for frequently accessed data.\n",
            "\n",
            "In contrast, GPUs have a different cache hierarchy. The L1 cache in a GPU is shared among all the cores, making it smaller than the L1 cache in CPUs. The L2 cache in GPUs is also shared among all the cores and is larger than the L2 cache in CPUs. The L3 cache in GPUs is not typically found in the same way as in CPUs; instead, the GPU's memory system is designed to be more efficient by using a combination of shared memory and registers that are directly accessible by the cores.\n",
            "\n",
            "\n",
            "Answers  :  64% 9/14 [00:59<00:39,  7.95s/it]\u001b[A\u001b[AText prompt : <|im_start|>system\n",
            "You are a helpful assistant.<|im_end|>\n",
            "<|im_start|>user\n",
            "<|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|>\n",
            "Look at the images and briefly answer the following question:\n",
            "1. What are the main computational units depicted in the hardware view of a GPU?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "Model question: 1. What are the main computational units depicted in the hardware view of a GPU?\n",
            "Model answer: The main computational units depicted in the hardware view of a GPU include the cores, L1 cache, L2 cache, and DRAM (Dynamic Random Access Memory).\n",
            "\n",
            "\n",
            "Answers  :  93% 13/14 [01:08<00:04,  4.90s/it]\u001b[A\u001b[AText prompt : <|im_start|>system\n",
            "You are a helpful assistant.<|im_end|>\n",
            "<|im_start|>user\n",
            "<|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|>\n",
            "Look at the images and briefly answer the following question:\n",
            "2. How does the software view of a GPU illustrate the organization of computational units into grids and blocks?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "Model question: 2. How does the software view of a GPU illustrate the organization of computational units into grids and blocks?\n",
            "Model answer: The software view of a GPU in the third image shows how computational units, specifically threads, are organized into grids and blocks. Each block is a collection of threads that work together on a specific part of the problem, while multiple blocks form a grid. This organization allows for efficient parallel processing by enabling different blocks to work independently and concurrently.\n",
            "\n",
            "\n",
            "Answers  : 100% 14/14 [01:19<00:00,  5.91s/it]\u001b[A\u001b[A\n",
            "\n",
            "                                              \u001b[A\u001b[A\n",
            "Questions:  12% 1/8 [01:36<11:17, 96.72s/it]\u001b[AQuerying model on: ['./presentation/3.jpeg', './presentation/4.jpeg', './presentation/5.jpeg']\n",
            "\n",
            "\n",
            "Answers  :   0% 0/14 [00:00<?, ?it/s]\u001b[A\u001b[AText prompt : <|im_start|>system\n",
            "You are a helpful assistant.<|im_end|>\n",
            "<|im_start|>user\n",
            "<|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|>\n",
            "Look at the images and briefly answer the following question:\n",
            "1. How does automatic scalability in CUDA work with respect to GPU architecture?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "Model question: 1. How does automatic scalability in CUDA work with respect to GPU architecture?\n",
            "Model answer: Automatic scalability in CUDA allows for efficient utilization of multiple Streaming Multiprocessors (SMs) on a GPU by distributing work across them. The image shows how a single thread block can be split into smaller blocks that can run concurrently on different SMs, maximizing the use of available resources. This enables better performance as more threads can be executed simultaneously, leading to faster execution times.\n",
            "\n",
            "\n",
            "Answers  :  21% 3/14 [00:11<00:40,  3.70s/it]\u001b[A\u001b[AText prompt : <|im_start|>system\n",
            "You are a helpful assistant.<|im_end|>\n",
            "<|im_start|>user\n",
            "<|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|>\n",
            "Look at the images and briefly answer the following question:\n",
            "2. What role do shared memory banks play in achieving automatic scalability on GPUs?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "Model question: 2. What role do shared memory banks play in achieving automatic scalability on GPUs?\n",
            "Model answer: Shared memory banks play a crucial role in achieving automatic scalability on GPUs by allowing threads within a block to share data efficiently. This reduces the need for expensive global memory accesses, which can be slow compared to local memory access. By using shared memory, threads can access frequently used data more quickly, leading to improved performance and better scalability as the number of threads increases.\n",
            "\n",
            "\n",
            "Answers  :  29% 4/14 [00:22<01:00,  6.03s/it]\u001b[A\u001b[AText prompt : <|im_start|>system\n",
            "You are a helpful assistant.<|im_end|>\n",
            "<|im_start|>user\n",
            "<|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|>\n",
            "Look at the images and briefly answer the following question:\n",
            "1. How does the hardware view of memory access differ from the software view in CUDA programming?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "Model question: 1. How does the hardware view of memory access differ from the software view in CUDA programming?\n",
            "Model answer: In the hardware view, memory access is depicted as a direct interaction between threads and the memory hierarchy (L2 cache, global memory). The software view, on the other hand, shows memory access through the perspective of thread blocks and clusters, which abstracts the actual hardware interactions for easier programming. This abstraction allows developers to focus on the logical organization of data and computations rather than the physical memory layout.\n",
            "\n",
            "\n",
            "Answers  :  57% 8/14 [00:33<00:24,  4.03s/it]\u001b[A\u001b[AText prompt : <|im_start|>system\n",
            "You are a helpful assistant.<|im_end|>\n",
            "<|im_start|>user\n",
            "<|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|>\n",
            "Look at the images and briefly answer the following question:\n",
            "2. What is the significance of the 32B or 128B consecutive bytes mentioned in the memory access explanation?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "Model question: 2. What is the significance of the 32B or 128B consecutive bytes mentioned in the memory access explanation?\n",
            "Model answer: The 32B or 128B consecutive bytes refer to the size of data that can be moved in a single memory access operation on a GPU. This is significant because it affects how efficiently data can be transferred between the GPU's registers and global memory, which impacts performance. If a thread needs to access multiple elements of an array, moving them in larger chunks (32B or 128B) can reduce the number of memory accesses required, leading to faster execution times.\n",
            "\n",
            "\n",
            "Answers  :  64% 9/14 [00:47<00:29,  5.83s/it]\u001b[A\u001b[AText prompt : <|im_start|>system\n",
            "You are a helpful assistant.<|im_end|>\n",
            "<|im_start|>user\n",
            "<|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|>\n",
            "Look at the images and briefly answer the following question:\n",
            "1. What is the purpose of the NVIDIA CUDA Compiler (nvcc) in the compilation path?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "Model question: 1. What is the purpose of the NVIDIA CUDA Compiler (nvcc) in the compilation path?\n",
            "Model answer: The NVIDIA CUDA Compiler (nvcc) is responsible for translating host C/C++ code into device assembly code, which can be executed on the GPU. It also handles the compilation of device just-in-time (JIT) code, which is generated during runtime to optimize performance.\n",
            "\n",
            "\n",
            "Answers  :  93% 13/14 [00:57<00:04,  4.07s/it]\u001b[A\u001b[AText prompt : <|im_start|>system\n",
            "You are a helpful assistant.<|im_end|>\n",
            "<|im_start|>user\n",
            "<|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|>\n",
            "Look at the images and briefly answer the following question:\n",
            "2. How does the host C/C++ code interact with the device assembly code during compilation?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "Model question: 2. How does the host C/C++ code interact with the device assembly code during compilation?\n",
            "Model answer: The host C/C++ code interacts with the device assembly code through the NVIDIA CUDA Compiler (nvcc). The nvcc compiler translates the host C/C++ code into device assembly code, which is then executed on the GPU. This process involves several steps:\n",
            "\n",
            "1. **Host C/C++ Code**: The programmer writes the application in C/C++, which includes both host code and device code.\n",
            "2. **nvcc Compilation**: The nvcc compiler takes the host C/C++ code as input and compiles it into device assembly code that can be executed on the GPU.\n",
            "3. **Device Assembly Code**: The device assembly code generated by nvcc is specific to the GPU architecture and is optimized for parallel execution.\n",
            "\n",
            "This interaction ensures that the host code communicates with the device code efficiently, allowing the GPU to perform computations in parallel.\n",
            "\n",
            "\n",
            "Answers  : 100% 14/14 [01:15<00:00,  6.32s/it]\u001b[A\u001b[A\n",
            "\n",
            "                                              \u001b[A\u001b[A\n",
            "Questions:  25% 2/8 [03:09<09:27, 94.56s/it]\u001b[AQuerying model on: ['./presentation/6.jpeg', './presentation/7.jpeg', './presentation/8.jpeg']\n",
            "\n",
            "\n",
            "Answers  :   0% 0/14 [00:00<?, ?it/s]\u001b[A\u001b[AText prompt : <|im_start|>system\n",
            "You are a helpful assistant.<|im_end|>\n",
            "<|im_start|>user\n",
            "<|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|>\n",
            "Look at the images and briefly answer the following question:\n",
            "1. What are the key directives used in CUDA programming as shown in the slide?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "Model question: 1. What are the key directives used in CUDA programming as shown in the slide?\n",
            "Model answer: The key directives used in CUDA programming, as shown in the first image, include:\n",
            "\n",
            "- `__global__`: Declares host/device callable kernels.\n",
            "- `__shared__`: Declares shared memory allocations.\n",
            "- `__device__`: Declares device callable kernels.\n",
            "- `<<<gridDim, blockDim>>>`: Kernel directives to specify grid/block dimensions.\n",
            "\n",
            "These directives help in defining the structure of the CUDA kernel functions and their execution on the GPU.\n",
            "\n",
            "\n",
            "Answers  :  21% 3/14 [00:12<00:45,  4.17s/it]\u001b[A\u001b[AText prompt : <|im_start|>system\n",
            "You are a helpful assistant.<|im_end|>\n",
            "<|im_start|>user\n",
            "<|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|>\n",
            "Look at the images and briefly answer the following question:\n",
            "2. How do the `__global__` and `__device__` directives differ in their usage within CUDA kernels?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "Model question: 2. How do the `__global__` and `__device__` directives differ in their usage within CUDA kernels?\n",
            "Model answer: The `__global__` directive is used to declare host/device callable kernels, which can be called from both the host (CPU) and device (GPU). On the other hand, the `__device__` directive is used to declare device callable kernels that can only be called from the GPU.\n",
            "\n",
            "\n",
            "Answers  :  29% 4/14 [00:22<01:01,  6.12s/it]\u001b[A\u001b[AText prompt : <|im_start|>system\n",
            "You are a helpful assistant.<|im_end|>\n",
            "<|im_start|>user\n",
            "<|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|>\n",
            "Look at the images and briefly answer the following question:\n",
            "1. What is Point-wise Polynomial Approximation (PwPA) and how does it work according to the slide?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "Model question: 1. What is Point-wise Polynomial Approximation (PwPA) and how does it work according to the slide?\n",
            "Model answer: Point-wise Polynomial Approximation (PwPA) is a method used to approximate an arbitrary function \\( p(x) \\). It works by partitioning the x-axis into subintervals, then approximating each partition with a polynomial of degree D and saving the polynomial coefficients \\( a_0,..., a_D \\). The coefficients approximate the function within their respective partitions.\n",
            "\n",
            "\n",
            "Answers  :  57% 8/14 [00:34<00:23,  3.99s/it]\u001b[A\u001b[AText prompt : <|im_start|>system\n",
            "You are a helpful assistant.<|im_end|>\n",
            "<|im_start|>user\n",
            "<|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|>\n",
            "Look at the images and briefly answer the following question:\n",
            "2. What are the main components of the PwPA method described in the slide?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "Model question: 2. What are the main components of the PwPA method described in the slide?\n",
            "Model answer: The main components of the PwPA (Point-wise Polynomial Approximation) method described in the slide include:\n",
            "\n",
            "1. **Partition-wise divide the x-axis into subintervals**: This involves dividing the range of the input variable `x` into smaller intervals or partitions.\n",
            "\n",
            "2. **Coefficients approximation**: For each partition, approximate the function using a polynomial of degree D. The coefficients of these polynomials are saved for later use.\n",
            "\n",
            "3. **Horner's method**: This is used to evaluate the polynomial within each partition efficiently. Horner's method reduces the number of multiplications required to evaluate a polynomial, making it more efficient.\n",
            "\n",
            "4. **Input-Output stationary algorithm**: This refers to the process of extracting partition IDs and partition coefficient pointers from the input data, which are then used to evaluate the polynomial within each partition.\n",
            "\n",
            "5. **Kernel function evaluation**: The kernel function evaluates the polynomial within each partition using the coefficients obtained earlier and returns the result.\n",
            "\n",
            "These components work together to approximate an arbitrary function `p(x)` by breaking it down into smaller, manageable parts (partitions), approximating each part with a polynomial, and then evaluating these polynomials efficiently using Horner's method.\n",
            "\n",
            "\n",
            "Answers  :  64% 9/14 [00:58<00:39,  7.84s/it]\u001b[A\u001b[AText prompt : <|im_start|>system\n",
            "You are a helpful assistant.<|im_end|>\n",
            "<|im_start|>user\n",
            "<|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|>\n",
            "Look at the images and briefly answer the following question:\n",
            "1. What is the purpose of the input-output stationary algorithm mentioned in the code snippet?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "Model question: 1. What is the purpose of the input-output stationary algorithm mentioned in the code snippet?\n",
            "Model answer: The input-output stationary algorithm mentioned in the code snippet is used to extract partition IDs and partition coefficient pointers from an arbitrary function p(x) using point-wise polynomial approximation. The algorithm partitions the x-axis into subintervals, approximates each partition with a polynomial of degree D, and saves the polynomial coefficients.\n",
            "\n",
            "\n",
            "Answers  :  93% 13/14 [01:08<00:05,  5.12s/it]\u001b[A\u001b[AText prompt : <|im_start|>system\n",
            "You are a helpful assistant.<|im_end|>\n",
            "<|im_start|>user\n",
            "<|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|>\n",
            "Look at the images and briefly answer the following question:\n",
            "2. How does the Horner scheme contribute to the evaluation of polynomials in the provided code?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "Model question: 2. How does the Horner scheme contribute to the evaluation of polynomials in the provided code?\n",
            "Model answer: The Horner scheme is used within the `evaluate_polynomials` function to evaluate polynomials efficiently. It reduces the number of multiplications required by evaluating the polynomial from the innermost coefficient outwards, which is particularly useful for large polynomials or when the coefficients are known beforehand. This method minimizes the computational cost by reducing the complexity of the polynomial evaluation process.\n",
            "\n",
            "\n",
            "Answers  : 100% 14/14 [01:20<00:00,  6.08s/it]\u001b[A\u001b[A\n",
            "\n",
            "                                              \u001b[A\u001b[A\n",
            "Questions:  38% 3/8 [04:47<08:00, 96.11s/it]\u001b[AQuerying model on: ['./presentation/9.jpeg', './presentation/10.jpeg', './presentation/11.jpeg']\n",
            "\n",
            "\n",
            "Answers  :   0% 0/14 [00:00<?, ?it/s]\u001b[A\u001b[AText prompt : <|im_start|>system\n",
            "You are a helpful assistant.<|im_end|>\n",
            "<|im_start|>user\n",
            "<|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|>\n",
            "Look at the images and briefly answer the following question:\n",
            "1. What metrics are being analyzed in the \"Source Counters\" section of the base kernel analysis?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "Model question: 1. What metrics are being analyzed in the \"Source Counters\" section of the base kernel analysis?\n",
            "Model answer: The \"Source Counters\" section of the base kernel analysis is analyzing metrics related to branch efficiency, warp stall reasons, and global access counts.\n",
            "\n",
            "\n",
            "Answers  :  21% 3/14 [00:07<00:28,  2.57s/it]\u001b[A\u001b[AText prompt : <|im_start|>system\n",
            "You are a helpful assistant.<|im_end|>\n",
            "<|im_start|>user\n",
            "<|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|>\n",
            "Look at the images and briefly answer the following question:\n",
            "2. How does the \"Warp State Statistics\" contribute to understanding the performance of the kernel execution?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "Model question: 2. How does the \"Warp State Statistics\" contribute to understanding the performance of the kernel execution?\n",
            "Model answer: The \"Warp State Statistics\" provides insights into how efficiently the warp is executing instructions, which directly impacts the overall performance of the kernel. By analyzing the number of cycles spent in various states (e.g., active threads per warp, predicted vs. non-predicted threads), developers can identify potential bottlenecks or inefficiencies in their code. This information helps them optimize the kernel for better performance by making necessary adjustments to the algorithm or data structure used within the kernel.\n",
            "\n",
            "\n",
            "Answers  :  29% 4/14 [00:20<00:58,  5.90s/it]\u001b[A\u001b[AText prompt : <|im_start|>system\n",
            "You are a helpful assistant.<|im_end|>\n",
            "<|im_start|>user\n",
            "<|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|>\n",
            "Look at the images and briefly answer the following question:\n",
            "1. What are the differences between the \"Code Layout\" and \"Memory Layout\" for Array of Structures (AoS) and Struct of Arrays (SoA)?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "Model question: 1. What are the differences between the \"Code Layout\" and \"Memory Layout\" for Array of Structures (AoS) and Struct of Arrays (SoA)?\n",
            "Model answer: The \"Code Layout\" refers to how data is organized in the code, while the \"Memory Layout\" refers to how data is stored in memory.\n",
            "\n",
            "For Array of Structures (AoS), the code layout organizes data as an array of structures, where each element contains multiple fields. In contrast, the memory layout for AoS stores all elements of a structure together before moving on to the next structure.\n",
            "\n",
            "In Struct of Arrays (SoA), the code layout organizes data as an array of arrays, where each array contains elements of the same type. The memory layout for SoA stores all elements of the same type consecutively before moving on to the next type.\n",
            "\n",
            "This difference impacts performance because AoS requires more memory accesses due to the need to jump between different structures, whereas SoA allows for contiguous memory access patterns, which can be more efficient.\n",
            "\n",
            "\n",
            "Answers  :  57% 8/14 [00:39<00:30,  5.15s/it]\u001b[A\u001b[AText prompt : <|im_start|>system\n",
            "You are a helpful assistant.<|im_end|>\n",
            "<|im_start|>user\n",
            "<|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|>\n",
            "Look at the images and briefly answer the following question:\n",
            "2. How do the data access patterns represented by AoS and SoA differ in terms of memory organization?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "Model question: 2. How do the data access patterns represented by AoS and SoA differ in terms of memory organization?\n",
            "Model answer: The data access patterns for Array of Structures (AoS) and Struct of Arrays (SoA) differ significantly in their memory organization. In AoS, each structure is stored as an array, meaning that all elements of a specific structure are stored consecutively in memory. This results in a more sequential memory layout where the data is accessed in a linear fashion.\n",
            "\n",
            "In contrast, SoA stores arrays of different structures together, with each element of a specific type being stored contiguously. This means that the memory layout for SoA is more interleaved, with elements of different types interspersed within the same block of memory. This can lead to better cache utilization but may require more complex memory access patterns during execution.\n",
            "\n",
            "\n",
            "Answers  :  64% 9/14 [00:56<00:36,  7.34s/it]\u001b[A\u001b[AText prompt : <|im_start|>system\n",
            "You are a helpful assistant.<|im_end|>\n",
            "<|im_start|>user\n",
            "<|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|>\n",
            "Look at the images and briefly answer the following question:\n",
            "1. What specific functions are shown in the code snippet for evaluating polynomials and calculating partitions using AoS and SoA?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "Model question: 1. What specific functions are shown in the code snippet for evaluating polynomials and calculating partitions using AoS and SoA?\n",
            "Model answer: The code snippet shows two functions: `evaluate_polynomials_AoS` and `evaluate_polynomials_SoA`. These functions evaluate plane polynomials and calculate partitions.\n",
            "\n",
            "\n",
            "Answers  :  93% 13/14 [01:04<00:04,  4.59s/it]\u001b[A\u001b[AText prompt : <|im_start|>system\n",
            "You are a helpful assistant.<|im_end|>\n",
            "<|im_start|>user\n",
            "<|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|>\n",
            "Look at the images and briefly answer the following question:\n",
            "2. How do the coefficients for the corresponding polynomial in the partition vary between AoS and SoA implementations?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "Model question: 2. How do the coefficients for the corresponding polynomial in the partition vary between AoS and SoA implementations?\n",
            "Model answer: In the AoS (Array of Structures) implementation, the coefficients for the corresponding polynomial in the partition are stored within each structure. In contrast, in the SoA (Structure of Arrays) implementation, these coefficients are stored as separate arrays, one for each coefficient. This means that in AoS, you would need to access multiple memory locations to retrieve all the coefficients for a single polynomial, while in SoA, you can directly access the array containing the coefficients for the specific polynomial.\n",
            "\n",
            "\n",
            "Answers  : 100% 14/14 [01:17<00:00,  5.92s/it]\u001b[A\u001b[A\n",
            "\n",
            "                                              \u001b[A\u001b[A\n",
            "Questions:  50% 4/8 [06:25<06:27, 96.82s/it]\u001b[AQuerying model on: ['./presentation/12.jpeg', './presentation/13.jpeg', './presentation/14.jpeg']\n",
            "\n",
            "\n",
            "Answers  :   0% 0/11 [00:00<?, ?it/s]\u001b[A\u001b[AText prompt : <|im_start|>system\n",
            "You are a helpful assistant.<|im_end|>\n",
            "<|im_start|>user\n",
            "<|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|>\n",
            "Look at the images and briefly answer the following question:\n",
            "1. What is the purpose of unrolling in the context of the PwPA algorithm shown in the code snippet?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "Model question: 1. What is the purpose of unrolling in the context of the PwPA algorithm shown in the code snippet?\n",
            "Model answer: Unrolling in the context of the PwPA algorithm shown in the code snippet refers to the process of expanding the loop iterations to reduce the number of branches and improve performance. By unrolling the loop, the compiler can eliminate the need for branching instructions, which can lead to better performance on certain hardware architectures.\n",
            "\n",
            "\n",
            "Answers  :  18% 2/11 [00:10<00:46,  5.15s/it]\u001b[A\u001b[AText prompt : <|im_start|>system\n",
            "You are a helpful assistant.<|im_end|>\n",
            "<|im_start|>user\n",
            "<|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|>\n",
            "Look at the images and briefly answer the following question:\n",
            "2. How does the unrolled loop structure in the code improve performance compared to a non-unrolled version?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "Model question: 2. How does the unrolled loop structure in the code improve performance compared to a non-unrolled version?\n",
            "Model answer: The unrolled loop structure improves performance by reducing the number of branches that need to be checked during execution, which can lead to better instruction-level parallelism (ILP) and reduced branch misprediction penalties. In the unrolled version, the if-branches are non-divergent, meaning they do not diverge into multiple paths based on certain conditions. This allows the compiler to optimize the code more effectively, as it can predict the flow of control with higher certainty. As a result, the unrolled version is likely to execute faster than the non-unrolled version because it reduces the overhead associated with branching and increases the likelihood of ILP utilization.\n",
            "\n",
            "\n",
            "Answers  :  27% 3/11 [00:25<01:16,  9.54s/it]\u001b[A\u001b[AText prompt : <|im_start|>system\n",
            "You are a helpful assistant.<|im_end|>\n",
            "<|im_start|>user\n",
            "<|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|>\n",
            "Look at the images and briefly answer the following question:\n",
            "1. What does the term \"non-divergent if-branches\" imply in the context of the PwPA algorithm?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "Model question: 1. What does the term \"non-divergent if-branches\" imply in the context of the PwPA algorithm?\n",
            "Model answer: In the context of the PwPA algorithm, non-divergent if-branches refer to conditional statements that do not cause divergent execution paths within a warp. This means that all threads within a warp will execute the same branch of the if-branch, ensuring that they all proceed in sync with each other. This is important for maintaining coherency and maximizing performance on GPU architectures where threads within a warp must be synchronized.\n",
            "\n",
            "\n",
            "Answers  :  55% 6/11 [00:38<00:30,  6.05s/it]\u001b[A\u001b[AText prompt : <|im_start|>system\n",
            "You are a helpful assistant.<|im_end|>\n",
            "<|im_start|>user\n",
            "<|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|>\n",
            "Look at the images and briefly answer the following question:\n",
            "2. How can the use of non-divergent if-branches enhance the efficiency of the algorithm as depicted in the code?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "Model question: 2. How can the use of non-divergent if-branches enhance the efficiency of the algorithm as depicted in the code?\n",
            "Model answer: The use of non-divergent if-branches enhances the efficiency of the algorithm by reducing the number of divergent branches, which can lead to stalls in the execution of the warp. In the provided code, the non-divergent if-branch is used to determine the partition index based on the value of `x_value`. This approach ensures that all threads within the same warp follow the same path through the code, thereby avoiding stalls caused by divergent branches.\n",
            "\n",
            "\n",
            "Answers  :  64% 7/11 [00:51<00:30,  7.64s/it]\u001b[A\u001b[AText prompt : <|im_start|>system\n",
            "You are a helpful assistant.<|im_end|>\n",
            "<|im_start|>user\n",
            "<|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|>\n",
            "Look at the images and briefly answer the following question:\n",
            "1. What metrics are used to analyze the performance of the SoA (Structure of Arrays) implementation in the PwPA kernel?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "Model question: 1. What metrics are used to analyze the performance of the SoA (Structure of Arrays) implementation in the PwPA kernel?\n",
            "Model answer: The metrics used to analyze the performance of the SoA implementation in the PwPA kernel include Warp Cycles Per Instruction Issued, Avg. Active Threads for Warp, Avg. Non-Preempted Threads for Warp, and Avg. Stall Cycles for Warp.\n",
            "\n",
            "\n",
            "Answers  :  91% 10/11 [01:00<00:05,  5.45s/it]\u001b[A\u001b[AText prompt : <|im_start|>system\n",
            "You are a helpful assistant.<|im_end|>\n",
            "<|im_start|>user\n",
            "<|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|>\n",
            "Look at the images and briefly answer the following question:\n",
            "2. How do the stall reasons highlighted in the analysis impact the overall performance of the kernel?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "Model question: 2. How do the stall reasons highlighted in the analysis impact the overall performance of the kernel?\n",
            "Model answer: The stall reasons highlighted in the analysis, such as \"not_selected\" and \"math_pipe_throttle,\" indicate that the kernel is not fully utilizing its resources or is experiencing delays due to pipeline stalls. These stalls can lead to reduced throughput and increased latency, which ultimately impacts the overall performance of the kernel. By addressing these stall reasons, the performance of the kernel can be improved.\n",
            "\n",
            "\n",
            "Answers  : 100% 11/11 [01:12<00:00,  6.62s/it]\u001b[A\u001b[A\n",
            "\n",
            "                                              \u001b[A\u001b[A\n",
            "Questions:  62% 5/8 [07:58<04:46, 95.49s/it]\u001b[AQuerying model on: ['./presentation/15.jpeg', './presentation/16.jpeg', './presentation/17.jpeg']\n",
            "\n",
            "\n",
            "Answers  :   0% 0/11 [00:00<?, ?it/s]\u001b[A\u001b[AText prompt : <|im_start|>system\n",
            "You are a helpful assistant.<|im_end|>\n",
            "<|im_start|>user\n",
            "<|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|>\n",
            "Look at the images and briefly answer the following question:\n",
            "1. How does the use of registers in the code improve data reuse and performance in the context of PwPA optimization?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "Model question: 1. How does the use of registers in the code improve data reuse and performance in the context of PwPA optimization?\n",
            "Model answer: The use of registers in the code improves data reuse by reducing the number of memory accesses required to perform calculations. By storing frequently accessed data in registers, the code can access this data more quickly than from main memory, which leads to faster execution times. This is particularly beneficial in the context of PwPA optimization as it allows for better utilization of the GPU's resources and improved overall performance.\n",
            "\n",
            "\n",
            "Answers  :  18% 2/11 [00:11<00:51,  5.76s/it]\u001b[A\u001b[AText prompt : <|im_start|>system\n",
            "You are a helpful assistant.<|im_end|>\n",
            "<|im_start|>user\n",
            "<|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|>\n",
            "Look at the images and briefly answer the following question:\n",
            "2. What specific operations in the code are highlighted as examples of data reuse with registers?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "Model question: 2. What specific operations in the code are highlighted as examples of data reuse with registers?\n",
            "Model answer: The specific operations highlighted as examples of data reuse with registers are `a += b` and `c += d`.\n",
            "\n",
            "\n",
            "Answers  :  27% 3/11 [00:18<00:51,  6.39s/it]\u001b[A\u001b[AText prompt : <|im_start|>system\n",
            "You are a helpful assistant.<|im_end|>\n",
            "<|im_start|>user\n",
            "<|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|>\n",
            "Look at the images and briefly answer the following question:\n",
            "1. In what ways do kernel calls in host code contribute to the implementation of shared memory in PwPA?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "Model question: 1. In what ways do kernel calls in host code contribute to the implementation of shared memory in PwPA?\n",
            "Model answer: Kernel calls in host code contribute to the implementation of shared memory in PwPA by enabling communication between the host and device, allowing the host to manage and control the execution of kernels on the device. This enables the efficient sharing of data between the host and device, which is essential for implementing shared memory in PwPA.\n",
            "\n",
            "\n",
            "Answers  :  55% 6/11 [00:29<00:22,  4.57s/it]\u001b[A\u001b[AText prompt : <|im_start|>system\n",
            "You are a helpful assistant.<|im_end|>\n",
            "<|im_start|>user\n",
            "<|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|>\n",
            "Look at the images and briefly answer the following question:\n",
            "2. How does the shared memory approach in this code segment differ from the register-based data reuse discussed earlier?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "Model question: 2. How does the shared memory approach in this code segment differ from the register-based data reuse discussed earlier?\n",
            "Model answer: The shared memory approach differs from the register-based data reuse by utilizing a different method for managing memory access. In the register-based data reuse, the code uses registers to store frequently accessed data to minimize memory access times. On the other hand, the shared memory approach involves using a shared memory region that is accessible by multiple threads within a block. This allows for more efficient data sharing between threads without the need for explicit synchronization mechanisms like locks or barriers.\n",
            "\n",
            "\n",
            "Answers  :  64% 7/11 [00:42<00:25,  6.44s/it]\u001b[A\u001b[AText prompt : <|im_start|>system\n",
            "You are a helpful assistant.<|im_end|>\n",
            "<|im_start|>user\n",
            "<|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|>\n",
            "Look at the images and briefly answer the following question:\n",
            "1. What metrics are used to evaluate the speedup achieved by the optimizations implemented in PwPA?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "Model question: 1. What metrics are used to evaluate the speedup achieved by the optimizations implemented in PwPA?\n",
            "Model answer: The image shows a comparison of different optimization techniques applied to PwPA, but it does not explicitly mention the specific metrics used for evaluating the speedup achieved by these optimizations. However, based on the context provided, it can be inferred that the metrics used might include execution time, memory usage, and overall performance improvement compared to the original implementation.\n",
            "\n",
            "\n",
            "Answers  :  91% 10/11 [00:53<00:05,  5.06s/it]\u001b[A\u001b[AText prompt : <|im_start|>system\n",
            "You are a helpful assistant.<|im_end|>\n",
            "<|im_start|>user\n",
            "<|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|>\n",
            "Look at the images and briefly answer the following question:\n",
            "2. How do the visualizations in the pie charts compare the performance of different versions of the algorithm before and after optimization?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "Model question: 2. How do the visualizations in the pie charts compare the performance of different versions of the algorithm before and after optimization?\n",
            "Model answer: The pie charts visually represent the performance improvement achieved by the optimized version of the algorithm compared to the original version. The size of each segment in the pie chart corresponds to the time taken for a specific operation or task, with smaller segments indicating faster execution times. By comparing the sizes of the segments between the original and optimized versions, one can see how much faster the optimized version performs certain operations.\n",
            "\n",
            "\n",
            "Answers  : 100% 11/11 [01:04<00:00,  6.34s/it]\u001b[A\u001b[A\n",
            "\n",
            "                                              \u001b[A\u001b[A\n",
            "Questions:  75% 6/8 [09:22<03:03, 91.59s/it]\u001b[AQuerying model on: ['./presentation/18.jpeg', './presentation/19.jpeg', './presentation/20.jpeg']\n",
            "\n",
            "\n",
            "Answers  :   0% 0/7 [00:00<?, ?it/s]\u001b[A\u001b[AText prompt : <|im_start|>system\n",
            "You are a helpful assistant.<|im_end|>\n",
            "<|im_start|>user\n",
            "<|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|>\n",
            "Look at the images and briefly answer the following question:\n",
            "1. What is the purpose of the \"torchPACE\" extension in PyTorch?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "Model question: 1. What is the purpose of the \"torchPACE\" extension in PyTorch?\n",
            "Model answer: The purpose of the \"torchPACE\" extension in PyTorch is to support instruction-level parallelism (ILP) for better performance in neural network training.\n",
            "\n",
            "\n",
            "Answers  :  29% 2/7 [00:08<00:20,  4.02s/it]\u001b[A\u001b[AText prompt : <|im_start|>system\n",
            "You are a helpful assistant.<|im_end|>\n",
            "<|im_start|>user\n",
            "<|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|>\n",
            "Look at the images and briefly answer the following question:\n",
            "2. How does the \"torchPACE\" extension enhance parallel processing capabilities in PyTorch?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "Model question: 2. How does the \"torchPACE\" extension enhance parallel processing capabilities in PyTorch?\n",
            "Model answer: The torchPACE extension enhances parallel processing capabilities in PyTorch by providing support for instruction-level parallelism (ILP) within a single thread, as shown in the second image. This allows for more efficient execution of operations within a single thread, which can lead to improved performance compared to traditional thread-level parallelism (TLP).\n",
            "\n",
            "\n",
            "Answers  :  43% 3/7 [00:18<00:27,  6.77s/it]\u001b[A\u001b[AText prompt : <|im_start|>system\n",
            "You are a helpful assistant.<|im_end|>\n",
            "<|im_start|>user\n",
            "<|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|>\n",
            "Look at the images and briefly answer the following question:\n",
            "1. What is the difference between thread-level parallelism (TLP) and instruction-level parallelism (ILP)?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "Model question: 1. What is the difference between thread-level parallelism (TLP) and instruction-level parallelism (ILP)?\n",
            "Model answer: Thread-level parallelism (TLP) involves using multiple threads to perform independent operations in parallel, while instruction-level parallelism (ILP) focuses on executing multiple instructions within a single thread simultaneously.\n",
            "\n",
            "\n",
            "Answers  :  86% 6/7 [00:27<00:04,  4.26s/it]\u001b[A\u001b[AText prompt : <|im_start|>system\n",
            "You are a helpful assistant.<|im_end|>\n",
            "<|im_start|>user\n",
            "<|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|>\n",
            "Look at the images and briefly answer the following question:\n",
            "2. How can instruction-level parallelism be utilized within a single thread to improve computational efficiency?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "Model question: 2. How can instruction-level parallelism be utilized within a single thread to improve computational efficiency?\n",
            "Model answer: Instruction-level parallelism (ILP) can be utilized within a single thread by executing multiple independent operations in parallel among instructions of a single thread. This means that instead of waiting for one operation to complete before starting another, the processor can execute several operations simultaneously. This increases the overall computational efficiency by maximizing the utilization of the available resources within a single thread.\n",
            "\n",
            "\n",
            "Answers  : 100% 7/7 [00:38<00:00,  5.83s/it]\u001b[A\u001b[A\n",
            "\n",
            "                                            \u001b[A\u001b[A\n",
            "Questions:  88% 7/8 [10:15<01:18, 78.73s/it]\u001b[AQuerying model on: []\n",
            "\n",
            "\n",
            "Answers  :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                    \u001b[A\u001b[A\n",
            "Documents: 100% 1/1 [10:19<00:00, 619.99s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explore 📚StudyFriend\n",
        "Try it yourself!\n",
        "\n",
        "- Change prompts via `--title_prompt`, `--question_prompt`, `--answer_prompt`.</br>\n",
        "- Change image size via `--image_size`.</br>\n",
        "- Change group size via `--group_size`.</br>\n",
        "\n",
        "And more! See the options below.\n",
        "\n",
        "**Note**: you can see default option values in the first part of a `--verbose` output.\n"
      ],
      "metadata": {
        "id": "gar82dKkMKg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m study_friend.query -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwln4k2jMfuz",
        "outputId": "ac37db83-f259-43a2-b4e8-146d6b0b94e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-02-24 23:03:12.562215: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1740438192.584257   17742 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1740438192.590505   17742 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-02-24 23:03:12.611769: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "usage: query.py [-h] [-d DIR] [-im IMAGE_SIZE] [-o OUTPUT_FILE] [-m MODEL]\n",
            "                [-e {transformers,mlx_vlm}] [-tp TITLE_PROMPT] [-qp QUESTION_PROMPT]\n",
            "                [-aq ANSWER_PROMPT] [-g GROUP_SIZE] [-t TEMPERATURE] [-mt MAX_TOKENS]\n",
            "                [-si SINGULAR_INJECTORS [SINGULAR_INJECTORS ...]]\n",
            "                [-pi PLURAL_INJECTORS [PLURAL_INJECTORS ...]] [-v]\n",
            "\n",
            "Creates a file containing question and answers about pdfs slides.\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "  -d DIR, --dir DIR     The directory from where pdfs are stored.\n",
            "  -im IMAGE_SIZE, --image_size IMAGE_SIZE\n",
            "                        The size of the images to resize to.\n",
            "  -o OUTPUT_FILE, --output_file OUTPUT_FILE\n",
            "                        The file to write the model response into.\n",
            "  -m MODEL, --model MODEL\n",
            "                        The model to query with.\n",
            "  -e {transformers,mlx_vlm}, --engine {transformers,mlx_vlm}\n",
            "                        Type of Engine to use, can be: transformers | mlx_vlm\n",
            "  -tp TITLE_PROMPT, --title_prompt TITLE_PROMPT\n",
            "                        Prompt to use to generate the title of a slidepack.\n",
            "  -qp QUESTION_PROMPT, --question_prompt QUESTION_PROMPT\n",
            "                        Prompt to use to generate questions.\n",
            "  -aq ANSWER_PROMPT, --answer_prompt ANSWER_PROMPT\n",
            "                        Prompt to use to generate answers.\n",
            "  -g GROUP_SIZE, --group_size GROUP_SIZE\n",
            "                        The size of the window to group images.\n",
            "  -t TEMPERATURE, --temperature TEMPERATURE\n",
            "                        The temperature to use for sampling.\n",
            "  -mt MAX_TOKENS, --max_tokens MAX_TOKENS\n",
            "                        The maximum number of tokens to generate.\n",
            "  -si SINGULAR_INJECTORS [SINGULAR_INJECTORS ...], --singular_injectors SINGULAR_INJECTORS [SINGULAR_INJECTORS ...]\n",
            "                        Array of singular injectors to replace pluralities in question prompt.\n",
            "  -pi PLURAL_INJECTORS [PLURAL_INJECTORS ...], --plural_injectors PLURAL_INJECTORS [PLURAL_INJECTORS ...]\n",
            "                        Array of plural injectors to be used when replace them in question prompt.\n",
            "  -v, --verbose         Enable verbose output.\n"
          ]
        }
      ]
    }
  ]
}